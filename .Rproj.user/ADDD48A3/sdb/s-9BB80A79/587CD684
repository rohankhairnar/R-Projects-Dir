{
    "collab_server" : "",
    "contents" : "#by rohan khairnar\n#Extracts details of products based on 'search_term'.\n#The 'search_term' is hard-coded and has to be entered in the R file before running.\n#Script runs in while loop for each 'next' occurence of a page, thus multiple iterations.\n#Final file contains data with product titles, selling price, ratings, review links, product links and product codes.\n\n#to do\n#mine for positive and negative reviews\n#code optimization, check\n#quality checks, NA replacements/alternate css selectors\n\n\nlibrary(xml2)\nlibrary(rvest)\nlibrary(utils)\n\nstart_time <- proc.time()\nlocal_file <- data.frame()\n\nsearch_term <- \"dell laptop 8GB 500\"\nsearch_term_coded <- URLencode(search_term)\n\nurl1 <- \"https://www.amazon.com\"\nurl2 <- \"/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=\"\namazon_pd <- paste0(url1, url2, search_term_coded)\npage <- 1\ndeco0 <-\"............................................................................\"\ndeco <- \"***********************\"\n\nwhile(!is.null(amazon_pd))\n{\n  while(amazon_pd != \"https://www.amazon.com\")\n  {\n    writeLines(paste0((\"\\n\"),deco,(\" Web Scrapping: Page \"),page,(\" for \"),search_term,deco,(\"\\n\")))\n    writeLines(paste0(\"Iteration \",page,\" initiated ! \\n\"))\n    amazon_html<- read_html(amazon_pd)\n    \n    writeLines(\"\\nFetching attributes....\\n.\\n.\\n.\")\n    attributes<- html_attrs(html_nodes(amazon_html,\"#resultsCol .s-access-detail-page\"))\n    writeLines(\"Attributes fetched\\n\\n\")\n    \n    writeLines(paste0((\"Iteration \"),page,(\" begins:\"),deco0,(\"\\n\")))\n    prod_titles <- sapply(attributes,'[[','title')\n    writeLines(paste0(length(prod_titles),\" products fetched\"))\n    prod_links <- sapply(attributes,'[[','href')\n    \n    #building valid links for missing domains. Some of the offer listed products have\n    #encoded urls, which need to be decoded and replaced\n    for(i in 1:length(prod_links))\n    {\n      if(length(prod_links[!grepl(url1,prod_links)]) != 0)\n      {\n        prod_links[i] <- gsub(\".*url=\",'',URLdecode(prod_links[i]))\n      }\n    }\n    \n    new_links <- prod_links\n    writeLines(paste0(length(new_links),\" links generated\"))\n\n    all_reviews_links <- gsub('/dp/','/product-reviews/',new_links)\n    writeLines(paste0(length(all_reviews_links), \" links for reviews generated\"))\n    \n    #fetching all product codes\n    prod_codes <- gsub('.*reviews/','',gsub('/ref.*','',all_reviews_links))\n    writeLines(paste0(length(prod_codes), \" product codes extracted\"))\n    \n    #parting the CSS Selectors in order to make it dynamic as per the product codes\n    part1 <- 'a.a-link-normal[href*='\n    part2 <- '] .sx-zero-spacing'\n    attributes_cost <- NULL\n    trial0 <- paste0(part1,prod_codes,part2)\n    prod_cost <- NULL\n    \n    for (i in 1:length(prod_codes))\n    {\n      if(length(html_nodes(amazon_html,trial0[i])) > 0)\n      {\n        attributes_cost[i] <-html_attrs(html_nodes(amazon_html,trial0[i]))\n        prod_cost[i] = sapply(attributes_cost[i],'[[','aria-label')\n      }\n      else\n      {\n        prod_cost[i] <- \"NA\"\n      }\n    }\n    \n    drop(part1)\n    drop(part2)\n    drop(trial0)\n    prod_cost = unlist(prod_cost)\n    writeLines(paste0(length(prod_cost), \" values for cost fetched\"))\n    \n    \n    part1 <- \"span[name*='\"\n    part2 <- \"'] span.a-icon-alt\"\n    trial0 <- paste0(part1,prod_codes,part2)\n    prod_rating <- NULL\n    \n    for (i in 1:length(prod_codes))\n    {\n      if(length(html_nodes(amazon_html,trial0[i])) > 0)\n      {\n        prod_rating[i] <-html_text(html_nodes(amazon_html,trial0[i]))\n        prod_rating[i] <- trimws(gsub('out.*','',prod_rating[i]))\n      }\n      else\n      {\n        prod_rating[i] <- \"NA\"\n      }\n    }\n    drop(part1)\n    drop(part2)\n    drop(trial0)\n    writeLines(paste0(length(prod_rating),\" product ratings fecthed\"))\n    \n    #for number of reviewers, to be added\n    #html_text(html_nodes(amazon_html,\".a-row a-spacing-top-mini span[name='B00YVVE7YO'] .a-size-small\"))\n    \n    #generate a data frame of new data and bind with local_file for all iterations\n    prod_data = data.frame(prod_titles,prod_cost,prod_rating,prod_codes,new_links,all_reviews_links)\n    \n    writeLines(\"\\nBinding the collected data...\\n.\\n.\")\n    local_file <- rbind(local_file,prod_data)\n    writeLines(\"Data sccessfully binded in local_file\\n\")\n    writeLines(paste0(\"Iteration \",page,\" ends\",deco0,\"\\n\\n\"))\n    \n    #for next iteration\n    next_pg <- html_attrs(html_nodes(amazon_html,\".pagnRA a\"))\n    next_pg_link <- paste0(url1,sapply(next_pg,'[[','href'))\n    \n    #used in next iteration\n    amazon_pd <-next_pg_link\n    page = page +1\n  }\n  file_name <- paste0(search_term,\".csv\")\n  write.csv(local_file, file = file_name)\n  writeLines(\"\\nFiles written successfully !\")\n  amazon_pd <- NULL\n  writeLines(paste0(\"\\nProcess completed !!\\n\"))\n  writeLines(paste0(\"Total products lined: \", length(local_file$prod_titles)))\n  writeLines(paste0(\"Total pages traversed: \", page))\n  writeLines(paste0(\"\\nResults for \",search_term,\" saved at \",getwd(),\"/\",file_name))\n  writeLines(paste0(\"\\nProcess time: \"))\n  et <- proc.time()-start_time\n  print(et)\n}\n",
    "created" : 1491329827931.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2121282554",
    "id" : "587CD684",
    "lastKnownWriteTime" : 1491777360,
    "last_content_update" : 1491777360194,
    "path" : "~/R Projects/Web Scraping with R/amazonsearch.R",
    "project_path" : "amazonsearch.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}